{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify from https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Embedding\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "path format :\n",
    "    - /img/ -- contain images\n",
    "    - /cache/ -- contain cache\n",
    "    - /caption.json -- contain caption\n",
    "    - /train.txt -- contain list of filename train\n",
    "    - /val.txt -- contain list of filename validation\n",
    "\n",
    "format caption.json :\n",
    "    - {\n",
    "        'filename': 000123.jpg\n",
    "        'captions': [\n",
    "            {'id': 'Bahasa Indonesia',\n",
    "             'en' : 'English'},\n",
    "            ...\n",
    "        ],\n",
    "        'emotions': {\n",
    "            'happy': 'Emosi happy',\n",
    "            'sad': 'Emosi sad',\n",
    "            'angry': 'Emosi angry'\n",
    "        }\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preparation import convert_mongo, split_and_save, load_caption\n",
    "\n",
    "path = './dataset'\n",
    "flickr_folder = path + '/flickr10k'\n",
    "coco_folder = path + '/mscoco10k'\n",
    "\n",
    "# # convert mongo to caption dict\n",
    "# caption_flickr, caption_coco = convert_mongo(path=path + 'caption_bc.json')\n",
    "\n",
    "# # save\n",
    "# with open(flickr_folder + '/caption.json', 'w') as f:\n",
    "#     json.dump(caption_flickr, f)\n",
    "# with open(coco_folder + '/caption.json', 'w') as f:\n",
    "#     json.dump(caption_coco, f)\n",
    "\n",
    "# split_and_save(caption_flickr, flickr_folder)\n",
    "# split_and_save(caption_coco, coco_folder)\n",
    "\n",
    "train, val = load_caption(flickr_folder)\n",
    "filenames_train, captions_train = train\n",
    "filenames_val, captions_val = val\n",
    "num_images_train = len(filenames_train)\n",
    "num_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Showing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from images import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(idx, folder_path, train):\n",
    "\n",
    "    if train:\n",
    "        filename = filenames_train[idx]\n",
    "        captions = captions_train[idx]\n",
    "    else:\n",
    "        filename = filenames_val[idx]\n",
    "        captions = captions_val[idx]\n",
    "\n",
    "    path = folder_path + '/img/' + filename\n",
    "\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "\n",
    "    img = load_image(path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_image(idx=0, folder_path=flickr_folder, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Image Model (ResNet152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderResNet152\n",
    "from keras import backend as K\n",
    "encoder_resnet152 = EncoderResNet152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = K.int_shape(encoder_resnet152.model.input)[1:3]\n",
    "transfer_values_size = K.int_shape(encoder_resnet152.model.output)[1]\n",
    "print('img_size', img_size)\n",
    "print('transfer_values_size', transfer_values_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from images import process_images_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transfer_values_train = process_images_all(folder_path=flickr_folder, \n",
    "                       is_train=True,\n",
    "                       filenames=filenames_train,\n",
    "                       img_size=img_size,\n",
    "                       transfer_values_size=transfer_values_size,\n",
    "                       image_model_transfer=encoder_resnet152.model,\n",
    "                       batch_size=64)\n",
    "print(\"dtype:\", transfer_values_train.dtype)\n",
    "print(\"shape:\", transfer_values_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transfer_values_val = process_images_all(folder_path=flickr_folder, \n",
    "                       is_train=False,\n",
    "                       filenames=filenames_val,\n",
    "                       img_size=img_size,\n",
    "                       transfer_values_size=transfer_values_size,\n",
    "                       image_model_transfer=encoder_resnet152.model,\n",
    "                       batch_size=64)\n",
    "print(\"dtype:\", transfer_values_val.dtype)\n",
    "print(\"shape:\", transfer_values_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import mark_captions, flatten, TokenizerWrap, mark_start, mark_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "captions_train_marked = mark_captions(captions_train)\n",
    "captions_train_flat = flatten(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = TokenizerWrap(texts=captions_train_flat,\n",
    "                          num_words=num_words)\n",
    "tokens_train = tokenizer.captions_to_tokens(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start = tokenizer.word_index[mark_start.strip()]\n",
    "token_end = tokenizer.word_index[mark_end.strip()]\n",
    "print('token_start', token_start)\n",
    "print('token_end', token_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = batch_generator(batch_size=batch_size,\n",
    "                            num_images_train=num_images_train,\n",
    "                            transfer_values_train=transfer_values_train,\n",
    "                            tokens_train=tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps Per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_captions_train = [len(captions) for captions in captions_train]\n",
    "total_num_captions_train = np.sum(num_captions_train)\n",
    "steps_per_epoch = int(total_num_captions_train / batch_size)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NIC and StyleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NIC, StyleNet\n",
    "state_size = 512\n",
    "embedding_size = 128\n",
    "factored_size = 256\n",
    "nic = NIC(num_words=num_words,\n",
    "          transfer_values_size=transfer_values_size,\n",
    "          state_size=state_size,\n",
    "          embedding_size=embedding_size)\n",
    "stylenet = StyleNet(mode='factual',\n",
    "                    num_words=num_words,\n",
    "                    transfer_values_size=transfer_values_size,\n",
    "                    state_size=state_size,\n",
    "                    embedding_size=embedding_size,\n",
    "                    factored_size=factored_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = 'checkpoints/checkpoint.id.keras'\n",
    "callback_checkpoint = ModelCheckpoint(stylenet,\n",
    "                                      filepath=path_checkpoint)\n",
    "callback_tensorboard = TensorBoard(log_dir='./logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_checkpoint, callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stylenet.load(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stylenet.model.fit_generator(generator=generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=20,\n",
    "                            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, max_tokens=30):\n",
    "    image = load_image(image_path, size=img_size)\n",
    "    image_batch = np.expand_dims(image, axis=0)\n",
    "    transfer_values = image_model_transfer.predict(image_batch)\n",
    "\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    token_int = token_start\n",
    "    output_text = ''\n",
    "    count_tokens = 0\n",
    "\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "        x_data = \\\n",
    "        {\n",
    "            'transfer_values_input': transfer_values,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "        decoder_output = decoder_model.predict(x_data)\n",
    "        \n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        sampled_word = tokenizer.token_to_word(token_int)\n",
    "\n",
    "        output_text += \" \" + sampled_word\n",
    "        \n",
    "        count_tokens += 1\n",
    "\n",
    "    output_tokens = decoder_input_data[0]\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Predicted caption:\")\n",
    "    print(output_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_caption(flickr_folder + '/img/' + filenames_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_caption(flickr_folder + '/img/' + filenames_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_all(folder_path, idx, train=False):\n",
    "    if train:\n",
    "        filename = filenames_train[idx]\n",
    "        captions = captions_train[idx]\n",
    "    else:\n",
    "        filename = filenames_val[idx]\n",
    "        captions = captions_val[idx]\n",
    "\n",
    "    path = folder_path + '/img/' + filename\n",
    "\n",
    "    generate_caption(path)\n",
    "\n",
    "    print(\"True captions:\")\n",
    "    for caption in captions:\n",
    "        print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_caption_all(folder_path=flickr_folder, idx=1, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption_all(folder_path=flickr_folder, idx=10, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption_all(folder_path=flickr_folder, idx=1, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import StyleNet, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
